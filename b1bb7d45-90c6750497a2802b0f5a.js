/*! For license information please see b1bb7d45-90c6750497a2802b0f5a.js.LICENSE.txt */
"use strict";(self.webpackChunkgatsby_starter_hello_world=self.webpackChunkgatsby_starter_hello_world||[]).push([[723],{5337:function(t,e,s){s.d(e,{QV:function(){return _}});var n=s(8081),i=s(9840),o=s(8891),r=s(8090),a=s(588),l=s(9897),h=s(3146),u=s(6275),c=s(7629),p=s(2328),f=s(8374),d=s(2931),g=s(618),m=s(6040),y=s(1977),w=s(7385),b=s(6325),T=s(69),v=s(8913),$=s(6347),z=s(6529);function O(t){return Array.isArray(t)}function S(t){return!function(t){return t instanceof n.Tensor}(t)&&!O(t)}function k(t,e,s,n=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(O(t)&&t.length>0)e=!0;else if(S(t)){for(const s in t)if(t.hasOwnProperty(s)){e=!0;break}}else e=!0;if(e)throw new a.nu(`Error when checking model ${i} expected no data, but got ${t}`)}return[]}if(null==t)return e.map((t=>null));let o;if(S(t)){o=[];for(const s of e){if(null==t[s])throw new a.nu(`No data provided for "${s}". Need data for each key in: ${e}`);o.push(t[s])}}else if(O(t)){if(t.length!==e.length)throw new a.nu(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): ${t}`);o=t}else{if(e.length>1)throw new a.nu(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${t.shape}`);o=[t]}if(o=(0,$.YV)(o),null!=s)for(let r=0;r<e.length;++r){if(null==s[r])continue;const t=o[r];if(t.shape.length!==s[r].length)throw new a.nu(`Error when checking ${i}: expected ${e[r]} to have ${s[r].length} dimension(s). but got array with shape ${t.shape}`);for(let e=0;e<s[r].length;++e){if(0===e&&!n)continue;const o=t.shape[e],l=s[r][e];if(null!=l&&l>=0&&o!==l)throw new a.nu(`${i} expected a batch of elements where each example has shape [${s[r].slice(1,s[r].length)}] (i.e.,tensor shape [*,${s[r].slice(1,s[r].length)}]) but the ${i} received an input with ${t.shape[0]} examples, each with shape [${t.shape.slice(1,t.shape.length)}] (tensor shape [${t.shape}])`)}}return o}function A(t,e,s,n=!0,i=""){let o;if(Array.isArray(t)){if(t.length!==e.length)throw new a.nu(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);o=t}else{if(e.length>1)throw new a.nu(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(t.shape)}.`);o=[t]}if(null!=s)for(let r=0;r<e.length;++r){if(null==s[r])continue;const t=o[r];if(t.shape.length!==s[r].length)throw new a.nu(`Error when checking ${i}: expected ${e[r]} to have ${s[r].length} dimension(s), but got array with shape ${JSON.stringify(t.shape)}`);for(let o=0;o<s[r].length;++o){if(0===o&&!n)continue;const l=t.shape[o],h=s[r][o];if(null!=h&&h!==l)throw new a.nu(`Error when checking ${i}: expected ${e[r]} to have shape ${JSON.stringify(s[r])} but got array with shape ${JSON.stringify(t.shape)}.`)}}}class _ extends b.W{constructor(t){super(t),this.isTraining=!1}summary(t,e,s=console.log){if(!this.built)throw new a.nu("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");(0,g.I)(this,t,e,s)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=p.j(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof n.Optimizer))throw new a.nu("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let e=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new a.nu(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const s=t.loss;e=s.map((t=>u.U2(t)))}else{const s=u.U2(t.loss);this.outputs.forEach((t=>{e.push(s)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new a.nu(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: ${this.outputNames}`);for(const s of this.outputNames)null==t.loss[s]&&console.warn(`Output "${s}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${s} during training`),e.push(u.U2(t.loss[s]))}this.lossFunctions=e,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let n=0;n<this.outputs.length;++n){const t=this.internalOutputShapes[n],e=this.outputNames[n];this.feedOutputNames.push(e),this.feedOutputShapes.push(t),this.feedLossFns.push(this.lossFunctions[n])}const s=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],(0,r.f4)("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const i=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let s;if("string"==typeof t||"function"==typeof t)s=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${t}`);s=t}if(Array.isArray(s))return e.map((t=>s));{const t=[];for(const n of e){let e=s.hasOwnProperty(n)?s[n]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),o=(t,e,s)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([s,t])};(0,r.f4)("metric",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;(e=>{let s,n,i;for(const a of e){if("string"==typeof a&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(a)){const e=this.internalOutputShapes[t];let o;1===e[e.length-1]||this.lossFunctions[t]===u.fO?-1!==["accuracy","acc"].indexOf(a)?n=c._F:-1!==["crossentropy","ce"].indexOf(a)&&(n=c.fO):this.lossFunctions[t]===u.KM?-1!==["accuracy","acc"].indexOf(a)?n=c.TY:-1!==["crossentropy","ce"].indexOf(a)&&(n=c.KM):-1!==["accuracy","acc"].indexOf(a)?n=c.G5:-1!==["crossentropy","ce"].indexOf(a)&&(n=c.uq),-1!==["accuracy","acc"].indexOf(a)?o="acc":-1!==["crossentropy","ce"].indexOf(a)&&(o="ce"),i=n,s=""+o}else{const t=c.U2(a);i=t,s=""+c.aI(a)}let e;(0,r.f4)(s,(()=>{e=i})),o(t,s,e)}})(i[t])}})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,s={}){const n=null==s.batchSize?32:s.batchSize;(0,$.fQ)(n);const i=this.standardizeUserDataXY(t,e,!0,n);try{const t=i[0].concat(i[1]);this.makeTestFunction();const e=this.testFunction,o=this.testLoop(e,t,n,s.verbose,s.steps);return(0,d.Bq)(o)}finally{(0,$.kS)(i[0],t),(0,$.kS)(i[1],e)}}async evaluateDataset(t,e){return this.makeTestFunction(),(0,v.D)(this,t,e)}checkNumSamples(t,e,s,n="steps"){let i;if(null!=s){if(i=null,null!=e)throw new a.nu(`If ${n} is set, batchSize must be null or undefined.Got batchSize = ${e}`)}else{if(null==t)throw new a.nu(`Either the input data should have a defined shape, or ${n} shoud be specified.`);i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,e){if(Array.isArray(e)&&0===e.length)throw new a.nu("`outputs` is an empty Array, which is not allowed.");const s=Array.isArray(e),i=s?e:[e],o=this.retrieveSymbolicTensors(i),r=new T.l2;if(t instanceof n.Tensor&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new a.nu(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)r.add(this.inputs[e],t[e])}else for(const n of this.inputs){const e=t[n.name];if(null==e)throw new a.nu(`No value is provided for the model's input ${n.name}`);r.add(n,e)}const l=(0,T.ht)(o,r);return s?l:l[0]}retrieveSymbolicTensors(t){const e=(0,d.JE)(null,t.length);let s=t.length;for(const n of this.layers){const i=Array.isArray(n.output)?n.output:[n.output],o=i.map((t=>t.name));for(let n=0;n<t.length;++n){const r=o.indexOf(t[n]);if(-1!==r&&(e[n]=i[r],s--),0===s)break}if(0===s)break}if(s>0){const s=[];throw e.forEach(((e,n)=>{null==e&&s.push(t[n])})),new a.nu(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(s)}`)}return e}predictLoop(t,e=32,s=!1){return n.tidy((()=>{const i=this.checkNumSamples(t);if(s)throw new a.nj("Verbose predictLoop() is not implemented yet.");const o=(0,$.R_)(i,e),r=this.outputs.map((t=>[]));for(let e=0;e<o.length;++e){n.tidy((()=>{const s=o[e][0],n=o[e][1],i=(0,$.sf)(t,s,n),r=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)r.push({key:this.inputs[t],value:i[t]});else r.push({key:this.inputs[0],value:i});const a=new T.l2(r);return(0,T.ht)(this.outputs,a)})).forEach(((t,e)=>r[e].push(t)))}return(0,d.Bq)(r.map((t=>n.concat(t,0))))}))}predict(t,e={}){const s=(0,$.YV)(t);A(s,this.inputNames,this.feedInputShapes,!1);try{const t=null==e.batchSize?32:e.batchSize;return(0,$.fQ)(t),this.predictLoop(s,t)}finally{(0,$.kS)(s,t)}}predictOnBatch(t){A(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,e,s=!0,i){if(null==this.optimizer_)throw new a.LH("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const o=[];for(let n=0;n<this.feedOutputShapes.length;++n){const t=this.feedOutputShapes[n];this.feedLossFns[n]===u.KM?o.push(t.slice(0,t.length-1).concat([1])):o.push(t)}if(function(t,e,s){const i=(0,d.Tw)(t.map((t=>t.shape[0])));i.sort();const o=(0,d.Tw)(e.map((t=>t.shape[0])));if(o.sort(),i.length>1)throw new a.nu(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(t.map((t=>t.shape)))}`);if(o.length>1)throw new a.nu(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(e.map((t=>t.shape)))}`);if(i.length>0&&o.length>0&&!n.util.arraysEqual(i,o))throw new a.nu(`Input Tensors should have the same number of samples as target Tensors. Found ${i[0]} input sample(s) and ${o[0]} target sample(s).`)}(t=k(t,this.feedInputNames,this.feedInputShapes,!1,"input"),e=k(e,this.feedOutputNames,o,!1,"target")),function(t,e,s){const n=[u.FD,u.fO,u.uq];for(let i=0;i<t.length;++i){const o=t[i],r=e[i],l=s[i];if(null!=r){if(r===u.uq&&1===o.shape[o.shape.length-1])throw new a.nu(`You are passing a target array of shape ${o.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==n.indexOf(r)){const t=o.shape.slice(1),e=l.slice(1);for(let s=0;s<t.length;++s){const n=t[s],i=e[s];if(null!=i&&n!==i)throw new a.nu(`A target Tensor with shape ${o.shape} was passed for an output of shape ${l}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(e,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!=0)throw new a.nu(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,e]}async standardizeUserData(t,e,s,n,i=!0,o){const[r,a]=this.standardizeUserDataXY(t,e,i,o);if(null!=s)throw new Error("sample weight is not supported yet.");let l=null;if(null!=n){const t=(0,z.Vf)(n,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await(0,z.tl)(a[e],null,t[e]))}return[r,a,l]}testLoop(t,e,s,o=0,r){return n.tidy((()=>{const l=this.checkNumSamples(e,s,r,"steps"),h=[];if(o>0)throw new a.nj("Verbose mode is not implemented yet.");if(null!=r)throw new a.nj("steps mode in testLoop() is not implemented yet");{const o=(0,$.R_)(l,s),r=(0,n.tensor1d)((0,m.w6)(0,l));for(let s=0;s<o.length;++s){const a=o[s][0],l=o[s][1],u=i.c9(r,a,l-a),c=(0,$.YX)(e,u),p=t(c);if(0===s)for(let t=0;t<p.length;++t)h.push((0,n.scalar)(0));for(let t=0;t<p.length;++t){const e=p[t];h[t]=n.add(h[t],n.mul(l-a,e))}}for(let t=0;t<h.length;++t)h[t]=n.div(h[t],l)}return h}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let s=0;s<t.length;++s){const n=t[s];let i=n;if((0,d.QX)(t,n)>1){i+=`_${(0,d.QX)(t.slice(0,s),n)}`}e.push(i)}return e}makeTrainFunction(){return t=>{const e=[],s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),o=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),r=[],a=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:s[e]});const a=new T.l2(t),l=(0,T.ht)(this.outputs,a,{training:!0});let h;for(let s=0;s<this.lossFunctions.length;++s){let t=(0,this.lossFunctions[s])(i[s],l[s]);null!=o[s]&&(t=(0,z.mo)(t,o[s]));const r=n.mean(t);e.push(r),h=0===s?t:n.add(h,t)}for(let s=0;s<this.metricsTensors.length;++s){let t;if(this.outputs.length>1&&s<this.outputs.length)t=e[s];else{const e=this.metricsTensors[s][0],o=this.metricsTensors[s][1];t=n.mean(e(i[o],l[o]))}n.keep(t),r.push(t)}return h=n.mean(h),this.calculateLosses().forEach((t=>{h=n.add(h,t)})),h}),!0,a)].concat(r)}}makeTestFunction(){this.testFunction=t=>n.tidy((()=>{const e=[];let s;const i=t.slice(0,this.inputs.length),o=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=[];for(let t=0;t<this.inputs.length;++t)r.push({key:this.inputs[t],value:i[t]});const a=new T.l2(r),l=(0,T.ht)(this.outputs,a);for(let t=0;t<this.lossFunctions.length;++t){const i=this.lossFunctions[t],r=n.mean(i(o[t],l[t]));s=0===t?r:n.add(s,r),e.push(s)}for(let t=0;t<this.metricsTensors.length;++t){const s=this.metricsTensors[t][0],i=this.metricsTensors[t][1],r=n.mean(s(o[i],l[i]));e.push(r)}return e}))}async fit(t,e,s={}){if(this.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let i,r,l,h,u,c,p,f,d;this.isTraining=!0;try{const n=null==s.batchSize?32:s.batchSize;(0,$.fQ)(n);const g=!1,m=await this.standardizeUserData(t,e,s.sampleWeight,s.classWeight,g,n);i=m[0],r=m[1],d=m[2];let y,w=!1;if(null!=s.validationData&&s.validationData.length>0){if(w=!0,2!==s.validationData.length)throw 3===s.validationData.length?new a.nj("validationData including sample weights is not supported yet."):new a.nu(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${s.validationData} is invalid.`);u=s.validationData[0],c=s.validationData[1];const t=!0,e=await this.standardizeUserData(u,c,null,null,t,n);p=e[0],f=e[1],y=p.concat(f)}else if(null!=s.validationSplit&&s.validationSplit>0&&s.validationSplit<1){w=!0;const t=Math.floor(i[0].shape[0]*(1-s.validationSplit)),e=i[0].shape[0];p=(0,$.sf)(i,t,e),l=i,i=(0,$.sf)(i,0,t),f=(0,$.sf)(r,t,e),h=r,r=(0,$.sf)(r,0,t),y=p.concat(f)}else null!=s.validationSteps&&(w=!0);const b=i.concat(r).concat(d);this.checkTrainableWeightsConsistency();const T=this.makeTrainFunction(),v=this.getDedupedMetricsNames();let z,O;w?(this.makeTestFunction(),z=this.testFunction,O=v.slice().concat(v.map((t=>"val_"+t)))):(z=null,y=[],O=v.slice());const S=(0,o.CZ)(s.callbacks,s.yieldEvery);return await this.fitLoop(T,b,v,n,s.epochs,s.verbose,S,z,y,s.shuffle,O,s.initialEpoch,null,null)}finally{this.isTraining=!1,(0,$.kS)(i,t),(0,$.kS)(r,e),(0,$.kS)(l,t),(0,$.kS)(h,e),(0,$.kS)(p,u),(0,$.kS)(f,c),null!=d&&n.dispose(d)}}async fitLoop(t,e,s,r,l,u,c,p,f,d,g,y,w,b){null==r&&(r=32),null==l&&(l=1),null==d&&(d=!0),null==y&&(y=0);let T=!1;if(null!=p&&null!=f&&(T=!0),null!=b&&(T=!0,null==w))throw new a.nu("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const v=this.checkNumSamples(e,r,w,"steps_per_epoch");let z;null!=v&&(z=(0,m.w6)(0,v)),null==u&&(u=1);const{callbackList:O,history:S}=(0,o.m$)(c,u,l,y,v,w,r,T,g);O.setModel(this),this.history=S,await O.onTrainBegin(),this.stopTraining_=!1;for(let o=y;o<l;++o){await O.onEpochBegin(o);const l={};if(null!=w)throw new a.nj("stepsPerEpoch mode is not implemented yet.");{if("batch"===d)throw new a.nj("batch shuffling is not implemneted yet");d&&n.util.shuffle(z);const o=(0,n.tensor1d)(z),u=(0,$.R_)(v,r);for(let a=0;a<u.length;++a){const c={};if(await O.onBatchBegin(a,c),n.tidy((()=>{const h=u[a][0],d=u[a][1],g=i.c9(o,h,d-h);c.batch=a,c.size=d-h;const m=(0,$.YX)(e,g),y=t(m);for(let t=0;t<s.length;++t){const e=s[t],i=y[t];c[e]=i,n.keep(i)}if(a===u.length-1&&T){const t=this.testLoop(p,f,r);for(let e=0;e<s.length;++e){const i=s[e],o=t[e];n.keep(o),l["val_"+i]=o}}})),await O.onBatchEnd(a,c),(0,h.i)(c),this.stopTraining_)break}o.dispose()}if(await O.onEpochEnd(o,l),this.stopTraining_)break}return await O.onTrainEnd(),await this.history.syncData(),this.history}async fitDataset(t,e){return(0,v.y)(this,t,e)}async trainOnBatch(t,e){const s=await this.standardizeUserData(t,e),i=s[0],o=s[1],r=this.makeTrainFunction()(i.concat(o)),a=[];for(const n of r){const t=await n.data();a.push(t[0])}return n.dispose(r),(0,$.kS)(s[0],t),(0,$.kS)(s[1],e),(0,d.Bq)(a)}getNamedWeights(t){const e=[],s=null!=t&&t.trainableOnly,n=s?this.trainableWeights:this.weights,i=this.getWeights(s);for(let o=0;o<n.length;++o)s&&!n[o].trainable||e.push({name:n[o].originalName,tensor:i[o]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const e=n.memory().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=e-n.memory().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=(0,d.D1)(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>(0,d.D1)(t)))}else{const e=Object.keys(this.loss);t={};const s=this.loss;for(const n of e){if("string"!=typeof s[n])throw new Error("Serialization of non-string loss is not supported.");t[n]=(0,d.D1)(s[n])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[(0,d.D1)(c.aI(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>(0,d.D1)(c.aI(t))));{const t={};for(const e in this.metrics)t[e]=(0,d.D1)(c.aI(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=(0,y.a)(t.optimizer_config),s=(0,l.v)(e);let n,i;if("string"==typeof t.loss)n=(0,d.zW)(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>(0,d.zW)(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=(0,d.zW)(t.loss[e])}if(Array.isArray(t.metrics))i=t.metrics.map((t=>(0,d.zW)(t)));else if(null!=t.metrics){i={};for(const e in t.metrics)i[e]=(0,d.zW)(t.metrics[e])}this.compile({loss:n,metrics:i,optimizer:s})}async save(t,e){if("string"==typeof t){const e=n.io.getSaveHandlers(t);if(0===e.length)throw new a.nu(`Cannot find any save handlers for URL '${t}'`);if(e.length>1)throw new a.nu(`Found more than one (${e.length}) save handlers for URL '${t}'`);t=e[0]}if(null==t.save)throw new a.nu("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const s=await n.io.encodeWeights(this.getNamedWeights(e)),i={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:`TensorFlow.js tfjs-layers v${w.i}`,convertedBy:null};if(null!=e&&e.includeOptimizer&&null!=this.optimizer){i.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:e,specs:o}=await n.io.encodeWeights(await this.optimizer.getWeights(),t);s.specs.push(...o),s.data=n.io.concatenateArrayBuffers([s.data,e])}if(null!=this.userDefinedMetadata){const t=!0;(0,f.WE)(this.userDefinedMetadata,this.name,t),i.userDefinedMetadata=this.userDefinedMetadata}return i.weightData=s.data,i.weightSpecs=s.specs,t.save(i)}setUserDefinedMetadata(t){(0,f.WE)(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}_.className="Model",n.serialization.registerClass(_);class N extends _{}N.className="Functional",n.serialization.registerClass(N)}}]);
//# sourceMappingURL=b1bb7d45-90c6750497a2802b0f5a.js.map